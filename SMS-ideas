The emulated processors are implemented in Scheme, and so to deal with chunk
data, the backing mmap'ed file memory must be copied to/from Scheme data
structures.  This happens to also emulate the real SMS in that chunks are copied
to/from processors.  So, there is the opportunity to emulate the processor-local
caching of the real design - by having a cache of chunks copied from the mmap'ed
file memory.

I want to minimize the amount of lower-level mechanisms of the real design that
are emulated, because I'm interested in exploring the higher-level semantics.
So, I think the emulator's chunk IDs will be simply offsets into the mmap'ed
file, because, unlike the real design, I think it's unnecessary to separate
chunk IDs from storage locations.  This eliminates the complexities of having in
the emulated SMS the mechanisms for associating chunk IDs with locations.  From
the programmer's point of view, the semantics should still be the same as if
chunk IDs were separated, because that view doesn't expose anything where it
makes a difference (I think).

The emulated processor will be a forked Ikarus process.  It will have a cache
(private, because forked processors won't shared memory) of chunks, implemented
in Scheme.  And it will have an "association unit" that is an associative array
mapping chunk IDs to their locations in the cache.


How is chunk ID allocation done?  Does allocating an ID also make that ID's
ref-count equal 1?

How is reference counting and GC done?

If multiple processes concurrently access the mapped storage file, all the usual
memory consistency issues apply.  Do atomic processor instructions work for
concurrency safety on shared mmap'ed memory?

Hmm, even though with the emulator it seems possible to have each
processor-process access the mmap'ed memory, this requires atomic instructions,
and it differs significantly from the real FB design.  It seems better to stay
closer to the real design, by having a dedicated SMS process that services the
processor processes by communicating over sockets (AF_UNIX, because it can
listen for new connections) or pipes (I suppose SMS process would have to fork
the processor processes so it can setup the pipes between them).  This design
also suggests making the processor processes "SMT" / hyper-threading, i.e. each
"processor" multiplexes multiple activate threads so that the processor stays
working on some threads while other threads are waiting for the SMS to fulfill
requests.  This seems better in the short-term, because it avoids
concurrent access issues, and it seems better in the long-term because the
emulator will be closer to the real design, which might be more valuable than I
realize now.

New Idea: Use Posix message queues.  They allow multiple senders and/or multiple
receivers using the same queue - multiple processor processes sending to one SMS
process.  Because an SMS process's reply is to a single processor process, it
seems each processor process needs its own queue to receive replies, which means
messages to the SMS must identify who it's from, and messages from the SMS must
know what queue to send to.  They allow independent processes to use the same
queue by opening it via a global string name.  Forking is not necessary for
multiple processes to use the queue, and it's not necessary for multiple
processes to share the mapped storage file.  However, if forking is not done,
separate Scheme programs must be run - is that better than forking?  Forking
seems better becaues it's simpler - no command-line, no input/output streams.
It seems possible to have the start-up process, before forking, open all the
message queues, then fork the SMS process which will have access to all the
queues as it needs, then fork the processor processes which will have access to
the the SMS queue and their queue and can close the others' queues (this
requires a loop counter variable so each forked child can know its identity to
know what queue to take).


